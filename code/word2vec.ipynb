{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf0fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f2c4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 21:58:11.480323: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-19 21:58:12.210946: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-19 21:58:12.211012: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-19 21:58:12.211019: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "163ff181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_squad(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "\n",
    "                if 'plausible_answers' in qa.keys():\n",
    "                    access = 'plausible_answers'\n",
    "                else:\n",
    "                    access = 'answers'\n",
    "                for answer in qa[access]:\n",
    "\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "    return contexts, questions, answers\n",
    "\n",
    "\n",
    "train_contexts, train_questions, train_answers = read_squad('squad/train-v2.0.json')\n",
    "val_contexts, val_questions, val_answers = read_squad('squad/dev-v2.0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad08d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "np.random.seed(42)\n",
    "train_contexts = np.array(train_contexts)\n",
    "train_questions = np.array(train_questions)\n",
    "train_answers = np.array(train_answers)\n",
    "\n",
    "val_contexts = np.array(val_contexts)\n",
    "val_questions = np.array(val_questions)\n",
    "val_answers = np.array(val_answers)\n",
    "\n",
    "# Get random indices for sampling\n",
    "train_indices = np.random.choice(len(train_contexts), 5000, replace=False)\n",
    "val_indices = np.random.choice(len(val_contexts), 500, replace=False)\n",
    "\n",
    "# Sample the data\n",
    "train_contexts_sampled = train_contexts[train_indices]\n",
    "train_questions_sampled = train_questions[train_indices]\n",
    "train_answers_sampled = train_answers[train_indices]\n",
    "\n",
    "val_contexts_sampled = val_contexts[val_indices]\n",
    "val_questions_sampled = val_questions[val_indices]\n",
    "val_answers_sampled = val_answers[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d033d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contexts_sampled = train_contexts_sampled.tolist()\n",
    "train_questions_sampled = train_questions_sampled.tolist()\n",
    "train_answers_sampled =train_answers_sampled.tolist()\n",
    "\n",
    "val_contexts_sampled = val_contexts_sampled.tolist()\n",
    "val_questions_sampled = val_questions_sampled.tolist()\n",
    "val_answers_sampled = val_answers_sampled.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cbd129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        else:\n",
    "            for n in [1, 2]:\n",
    "                if context[start_idx-n:end_idx-n] == gold_text:\n",
    "                    answer['answer_start'] = start_idx - n\n",
    "                    answer['answer_end'] = end_idx - n\n",
    "\n",
    "add_end_idx(train_answers_sampled, train_contexts_sampled)\n",
    "add_end_idx(val_answers_sampled, val_contexts_sampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb71a12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/kunuruabhishek/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1dd8200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/kunuruabhishek/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e87bd391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f32c130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stem=True):\n",
    "  text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "  tokens = []\n",
    "  for token in text.split():\n",
    "      if stem:\n",
    "        tokens.append(stemmer.stem(token))\n",
    "      else:\n",
    "        tokens.append(token)\n",
    "  return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8681b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def train_word2vec(sentences):\n",
    "    tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "    word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=300, window=5, min_count=1, workers=4)\n",
    "    return word2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53e68d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_word2vec_embeddings(sentences, model):\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        if isinstance(sentence, str):\n",
    "            words = sentence.split()  \n",
    "        elif isinstance(sentence, list):\n",
    "            sentence = ' '.join(sentence)\n",
    "            words = sentence.split()\n",
    "        else:\n",
    "            embeddings.append(np.zeros(model.vector_size))\n",
    "            continue\n",
    "        \n",
    "        word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "        if word_vectors:\n",
    "            sentence_vector = np.mean(word_vectors, axis=0)\n",
    "        else:\n",
    "            sentence_vector = np.zeros(model.vector_size)\n",
    "        \n",
    "        embeddings.append(sentence_vector)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def calculate_similarity_scores(context_sentences, question, model):\n",
    "    context_embeddings = get_word2vec_embeddings(context_sentences, model)\n",
    "    question_embedding = get_word2vec_embeddings([question], model)[0]\n",
    "    similarity_scores = cosine_similarity(context_embeddings, question_embedding.reshape(1, -1)).flatten()\n",
    "    return similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee1a20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_context_by_similarity(context_sentences, similarity_scores, threshold):\n",
    "    similarity_scores = np.array(similarity_scores)  \n",
    "    filtered_indices = np.where(similarity_scores > threshold)[0]\n",
    "    if len(filtered_indices) == 0:\n",
    "        return ' '.join(context_sentences)\n",
    "    filtered_indices = sorted(filtered_indices)\n",
    "    filtered_sentences = [context_sentences[i] for i in filtered_indices]\n",
    "    return '.'.join(filtered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ac7fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_squad_contexts(contexts, questions):\n",
    "    filtered_contexts = []\n",
    "    word2vec_model = train_word2vec(contexts)  # Train Word2Vec model on contexts\n",
    "    for context, question in tqdm(zip(contexts, questions), total=len(contexts)):\n",
    "#         word2vec_model = train_word2vec(context)\n",
    "        context_sentences = context.split('.')\n",
    "        preprocessed_sentences = [preprocess(sentence) for sentence in context_sentences]\n",
    "        preprocessed_question = preprocess(question)\n",
    "        similarity_scores = calculate_similarity_scores(preprocessed_sentences, preprocessed_question, word2vec_model)\n",
    "#         print(similarity_scores)\n",
    "        filtered_context = filter_context_by_similarity(context_sentences, similarity_scores, 0.5)\n",
    "#         print(filtered_context)\n",
    "        filtered_contexts.append(filtered_context)\n",
    "       \n",
    "    return filtered_contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4af7a268",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:09<00:00, 539.87it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 509.82it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filtered_train_contexts = filter_squad_contexts(train_contexts_sampled, train_questions_sampled)\n",
    "filtered_val_contexts = filter_squad_contexts(val_contexts_sampled, val_questions_sampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81a04198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 46 examples where the answer could not be found in the truncated context.\n"
     ]
    }
   ],
   "source": [
    "def align_answers_with_context(original_contexts, updated_contexts, answers):\n",
    "    new_contexts = []\n",
    "    aligned_answers = []\n",
    "    skipped_count = 0\n",
    "\n",
    "    for orig_context, updated_context, answer in zip(original_contexts, updated_contexts, answers):\n",
    "        start_pos = answer['answer_start']\n",
    "        end_pos = answer['answer_end']\n",
    "        orig_answer = orig_context[start_pos:end_pos]\n",
    "        start_idx = updated_context.find(orig_answer)\n",
    "\n",
    "        if start_idx == -1:\n",
    "            skipped_count += 1\n",
    "\n",
    "            modified_answer = answer.copy()\n",
    "            modified_answer['answer_start'] = len(updated_contexts)\n",
    "            modified_answer['answer_end'] = len(updated_contexts)\n",
    "        else:\n",
    "            new_start_pos = start_idx\n",
    "            new_end_pos = start_idx + len(orig_answer)\n",
    "            # Create a copy of answer and update positions\n",
    "            modified_answer = answer.copy()\n",
    "            modified_answer['answer_start'] = new_start_pos\n",
    "            modified_answer['answer_end'] = new_end_pos\n",
    "\n",
    "        aligned_answers.append(modified_answer)\n",
    "        new_contexts.append(updated_context)\n",
    "\n",
    "    return new_contexts, aligned_answers, skipped_count\n",
    "\n",
    "filtered_train_contexts, train_answers_sampled, skipped_count = align_answers_with_context(train_contexts_sampled, filtered_train_contexts, train_answers_sampled)\n",
    "print(f'Skipped {skipped_count} examples where the answer could not be found in the truncated context.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2901c8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 46 examples (0.92%) where the answer could not be found in the truncated context.\n"
     ]
    }
   ],
   "source": [
    "total_contexts = len(train_contexts_sampled) \n",
    "\n",
    "skipped_percentage = (skipped_count / total_contexts) * 100\n",
    "print(f'Skipped {skipped_count} examples ({skipped_percentage:.2f}%) where the answer could not be found in the truncated context.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c2835a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The first Sky television rights agreement was worth £304 million over five seasons. The next contract, negotiated to start from the 1997–98 season, rose to £670 million over four seasons. The third contract was a £1.024 billion deal with BSkyB for the three seasons from 2001–02 to 2003–04. The league brought in £320 million from the sale of its international rights for the three-year period from 2004–05 to 2006–07. It sold the rights itself on a territory-by-territory basis. Sky's monopoly was broken from August 2006 when Setanta Sports was awarded rights to show two out of the six packages of matches available. This occurred following an insistence by the European Commission that exclusive rights should not be sold to one television company. Sky and Setanta paid a total of £1.7 billion, a two-thirds increase which took many commentators by surprise as it had been widely assumed that the value of the rights had levelled off following many years of rapid growth. Setanta also hold rights to a live 3 pm match solely for Irish viewers. The BBC has retained the rights to show highlights for the same three seasons (on Match of the Day) for £171.6 million, a 63 per cent increase on the £105 million it paid for the previous three-year period. Sky and BT have agreed to jointly pay £84.3 million for delayed television rights to 242 games (that is the right to broadcast them in full on television and over the internet) in most cases for a period of 50 hours after 10 pm on matchday. Overseas television rights fetched £625 million, nearly double the previous contract. The total raised from these deals is more than £2.7 billion, giving Premier League clubs an average media income from league games of around £40 million-a-year from 2007 to 2010\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_train_contexts[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9bc3f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The first Sky television rights agreement was worth £304 million over five seasons. The next contract, negotiated to start from the 1997–98 season, rose to £670 million over four seasons. The third contract was a £1.024 billion deal with BSkyB for the three seasons from 2001–02 to 2003–04. The league brought in £320 million from the sale of its international rights for the three-year period from 2004–05 to 2006–07. It sold the rights itself on a territory-by-territory basis. Sky's monopoly was broken from August 2006 when Setanta Sports was awarded rights to show two out of the six packages of matches available. This occurred following an insistence by the European Commission that exclusive rights should not be sold to one television company. Sky and Setanta paid a total of £1.7 billion, a two-thirds increase which took many commentators by surprise as it had been widely assumed that the value of the rights had levelled off following many years of rapid growth. Setanta also hold rights to a live 3 pm match solely for Irish viewers. The BBC has retained the rights to show highlights for the same three seasons (on Match of the Day) for £171.6 million, a 63 per cent increase on the £105 million it paid for the previous three-year period. Sky and BT have agreed to jointly pay £84.3 million for delayed television rights to 242 games (that is the right to broadcast them in full on television and over the internet) in most cases for a period of 50 hours after 10 pm on matchday. Overseas television rights fetched £625 million, nearly double the previous contract. The total raised from these deals is more than £2.7 billion, giving Premier League clubs an average media income from league games of around £40 million-a-year from 2007 to 2010.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_contexts[train_indices[12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3b10167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How much many did the Premier League make from selling its internation rights during 2004-07?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions_sampled[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67241b2a",
   "metadata": {},
   "source": [
    "### After sentence selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1f47562",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train_contexts = list(filtered_train_contexts)\n",
    "train_questions_sampled = list(train_questions_sampled)\n",
    "filtered_val_contexts = list(filtered_val_contexts)\n",
    "val_questions_sampled = list(val_questions_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c0d24d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(filtered_train_contexts, train_questions_sampled, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(filtered_val_contexts, val_questions_sampled, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9160d16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForQuestionAnswering, AdamW\n",
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abd4e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_pos = encodings.char_to_token(i, answers[i]['answer_start'])\n",
    "        end_pos = encodings.char_to_token(i, answers[i]['answer_end'])\n",
    "\n",
    "        if start_pos is None:\n",
    "            start_pos = tokenizer.model_max_length\n",
    "        if end_pos is None:\n",
    "            shift = 1\n",
    "            while end_pos is None and answers[i]['answer_end'] - shift >= 0:\n",
    "                end_pos = encodings.char_to_token(i, answers[i]['answer_end'] - shift)\n",
    "                shift += 1\n",
    "        if end_pos is None:\n",
    "            end_pos = tokenizer.model_max_length\n",
    "\n",
    "        start_positions.append(start_pos)\n",
    "        end_positions.append(end_pos)\n",
    "        \n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers_sampled)\n",
    "add_token_positions(val_encodings, val_answers_sampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8cbc398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c02dd19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU: NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# setup GPU/CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Running on GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e1620f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunuruabhishek/anaconda3/envs/pytorch/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "Epoch 0: 100%|██████████| 313/313 [02:26<00:00,  2.14it/s, loss=2.02]\n",
      "Epoch 1: 100%|██████████| 313/313 [02:29<00:00,  2.09it/s, loss=1.48] \n",
      "Epoch 2: 100%|██████████| 313/313 [02:30<00:00,  2.08it/s, loss=0.62] \n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        start_positions=start_positions,\n",
    "                        end_positions=end_positions)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12f62fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4462890625"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "acc = []\n",
    "for batch in val_loader:\n",
    "    with torch.no_grad():\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "        acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
    "        acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n",
    "acc = sum(acc)/len(acc)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "313be739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def calculate_f1(pred_start, pred_end, true_start, true_end):\n",
    "    pred_tokens = set(range(pred_start, pred_end + 1))\n",
    "    true_tokens = set(range(true_start, true_end + 1))\n",
    "\n",
    "    common_tokens = pred_tokens.intersection(true_tokens)\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0, 0, 0  # precision, recall, f1\n",
    "\n",
    "    precision = len(common_tokens) / len(pred_tokens)\n",
    "    recall = len(common_tokens) / len(true_tokens)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b51464fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "start_acc = []\n",
    "end_acc = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "for batch in val_loader:\n",
    "    with torch.no_grad():\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "        start_acc.append((start_pred == start_true).sum().item() / len(start_pred))\n",
    "        end_acc.append((end_pred == end_true).sum().item() / len(end_pred))\n",
    "        for sp, ep, st, et in zip(start_pred, end_pred, start_true, end_true):\n",
    "            precision, recall, f1 = calculate_f1(sp.item(), ep.item(), st.item(), et.item())\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1s.append(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44b52193",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "avg_start_acc = sum(start_acc) / len(start_acc)\n",
    "avg_end_acc = sum(end_acc) / len(end_acc)\n",
    "avg_precision = sum(precisions) / len(precisions)\n",
    "avg_recall = sum(recalls) / len(recalls)\n",
    "avg_f1 = sum(f1s) / len(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e42e23a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Start Position Accuracy: 0.4375\n",
      "Average End Position Accuracy: 0.4551\n",
      "Average Precision: 0.4829\n",
      "Average Recall: 0.5876\n",
      "Average F1 Score: 0.4789\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Average Start Position Accuracy: {avg_start_acc:.4f}\")\n",
    "print(f\"Average End Position Accuracy: {avg_end_acc:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00610b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb3bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa273af3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
