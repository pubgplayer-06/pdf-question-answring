{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d935d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7da9cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 00:41:13.296035: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43f735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_squad(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "\n",
    "                if 'plausible_answers' in qa.keys():\n",
    "                    access = 'plausible_answers'\n",
    "                else:\n",
    "                    access = 'answers'\n",
    "                for answer in qa[access]:\n",
    "\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "    return contexts, questions, answers\n",
    "\n",
    "\n",
    "train_contexts, train_questions, train_answers = read_squad('squad/train-v2.0.json')\n",
    "val_contexts, val_questions, val_answers = read_squad('squad/dev-v2.0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d70e2adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "import numpy as np\n",
    "# from gensim.models import Word2Vec\n",
    "np.random.seed(42)\n",
    "train_contexts = np.array(train_contexts)\n",
    "train_questions = np.array(train_questions)\n",
    "train_answers = np.array(train_answers)\n",
    "\n",
    "val_contexts = np.array(val_contexts)\n",
    "val_questions = np.array(val_questions)\n",
    "val_answers = np.array(val_answers)\n",
    "\n",
    "\n",
    "train_indices = np.random.choice(len(train_contexts), 5000, replace=False)\n",
    "val_indices = np.random.choice(len(val_contexts), 500, replace=False)\n",
    "\n",
    "train_contexts_sampled = train_contexts[train_indices]\n",
    "train_questions_sampled = train_questions[train_indices]\n",
    "train_answers_sampled = train_answers[train_indices]\n",
    "\n",
    "val_contexts_sampled = val_contexts[val_indices]\n",
    "val_questions_sampled = val_questions[val_indices]\n",
    "val_answers_sampled = val_answers[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f025b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contexts_sampled = train_contexts_sampled.tolist()\n",
    "train_questions_sampled = train_questions_sampled.tolist()\n",
    "train_answers_sampled =train_answers_sampled.tolist()\n",
    "\n",
    "val_contexts_sampled = val_contexts_sampled.tolist()\n",
    "val_questions_sampled = val_questions_sampled.tolist()\n",
    "val_answers_sampled = val_answers_sampled.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc370801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        else:\n",
    "            for n in [1, 2]:\n",
    "                if context[start_idx-n:end_idx-n] == gold_text:\n",
    "                    answer['answer_start'] = start_idx - n\n",
    "                    answer['answer_end'] = end_idx - n\n",
    "\n",
    "add_end_idx(train_answers_sampled, train_contexts_sampled)\n",
    "add_end_idx(val_answers_sampled, val_contexts_sampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62e6cff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/kunuruabhishek/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fda88a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/kunuruabhishek/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5ace22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0257d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stem=True):\n",
    "  text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "  tokens = []\n",
    "  for token in text.split():\n",
    "    if token not in stop_words:\n",
    "      if stem:\n",
    "        tokens.append(stemmer.stem(token))\n",
    "      else:\n",
    "        tokens.append(token)\n",
    "  return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "556d7983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def calculate_similarity_scores(context_sentences, question):\n",
    "    documents = context_sentences + [question]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "    question_vector = tfidf_matrix[-1]\n",
    "    context_vectors = tfidf_matrix[:-1]\n",
    "\n",
    "    similarity_scores = cosine_similarity(question_vector, context_vectors).flatten()\n",
    "    return similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eaa1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def filter_context_by_similarity(context_sentences, similarity_scores, threshold):\n",
    "    similarity_scores = np.array(similarity_scores)  \n",
    "    \n",
    "    filtered_indices = np.where(similarity_scores > threshold)[0]\n",
    "    \n",
    "    if len(filtered_indices) == 0:\n",
    "        return ' '.join(context_sentences)\n",
    "    filtered_indices = sorted(filtered_indices)\n",
    "\n",
    "    filtered_sentences = [context_sentences[i] for i in filtered_indices]\n",
    "    return ' '.join(filtered_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8f21b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:13<00:00, 373.25it/s]\n",
      "100%|██████████| 500/500 [00:01<00:00, 373.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def filter_squad_contexts(contexts, questions):\n",
    "    filtered_contexts = []\n",
    "    for context, question in tqdm(zip(contexts, questions), total=len(contexts)):\n",
    "        context_sentences = context.split('.')  # Split into sentences based on full stops\n",
    "        preprocessed_sentences = [preprocess(sentence) for sentence in context_sentences]\n",
    "        preprocessed_question = preprocess(question)\n",
    "        similarity_scores = calculate_similarity_scores(preprocessed_sentences, preprocessed_question)\n",
    "        filtered_context = filter_context_by_similarity(context_sentences, similarity_scores, 0.05)\n",
    "        filtered_contexts.append(filtered_context)\n",
    "    return filtered_contexts\n",
    "\n",
    "filtered_train_contexts = filter_squad_contexts(train_contexts_sampled, train_questions_sampled)\n",
    "filtered_val_contexts = filter_squad_contexts(val_contexts_sampled, val_questions_sampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cffc8527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 489 examples where the answer could not be found in the truncated context.\n"
     ]
    }
   ],
   "source": [
    "def align_answers_with_context(original_contexts, updated_contexts, answers):\n",
    "    new_contexts = []\n",
    "    aligned_answers = []\n",
    "    skipped_count = 0\n",
    "\n",
    "    for orig_context, updated_context, answer in zip(original_contexts, updated_contexts, answers):\n",
    "        start_pos = answer['answer_start']\n",
    "        end_pos = answer['answer_end']\n",
    "        orig_answer = orig_context[start_pos:end_pos]\n",
    "        start_idx = updated_context.find(orig_answer)\n",
    "\n",
    "        if start_idx == -1:\n",
    "            skipped_count += 1\n",
    "\n",
    "            modified_answer = answer.copy()\n",
    "            modified_answer['answer_start'] = len(updated_contexts)\n",
    "            modified_answer['answer_end'] = len(updated_contexts)\n",
    "        else:\n",
    "            new_start_pos = start_idx\n",
    "            new_end_pos = start_idx + len(orig_answer)\n",
    "            modified_answer = answer.copy()\n",
    "            modified_answer['answer_start'] = new_start_pos\n",
    "            modified_answer['answer_end'] = new_end_pos\n",
    "\n",
    "        aligned_answers.append(modified_answer)\n",
    "        new_contexts.append(updated_context)\n",
    "\n",
    "    return new_contexts, aligned_answers, skipped_count\n",
    "\n",
    "filtered_train_contexts, train_answers_sampled, skipped_count = align_answers_with_context(train_contexts_sampled, filtered_train_contexts, train_answers_sampled)\n",
    "print(f'Skipped {skipped_count} examples where the answer could not be found in the truncated context.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5eee0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 489 examples (9.78%) where the answer could not be found in the truncated context.\n"
     ]
    }
   ],
   "source": [
    "total_contexts = len(train_contexts_sampled) \n",
    "\n",
    "skipped_percentage = (skipped_count / total_contexts) * 100\n",
    "print(f'Skipped {skipped_count} examples ({skipped_percentage:.2f}%) where the answer could not be found in the truncated context.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f67544ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It threatened the collapse of large financial institutions, which was prevented by the bailout of banks by national governments, but stock markets still dropped worldwide  dollars, and a downturn in economic activity leading to the 2008–2012 global recession and contributing to the European sovereign-debt crisis  The active phase of the crisis, which manifested as a liquidity crisis, can be dated from August 9, 2007, when BNP Paribas terminated withdrawals from three hedge funds citing \"a complete evaporation of liquidity\"'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_train_contexts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "351740bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It threatened the collapse of large financial institutions, which was prevented by the bailout of banks by national governments, but stock markets still dropped worldwide. In many areas, the housing market also suffered, resulting in evictions, foreclosures and prolonged unemployment. The crisis played a significant role in the failure of key businesses, declines in consumer wealth estimated in trillions of U.S. dollars, and a downturn in economic activity leading to the 2008–2012 global recession and contributing to the European sovereign-debt crisis. The active phase of the crisis, which manifested as a liquidity crisis, can be dated from August 9, 2007, when BNP Paribas terminated withdrawals from three hedge funds citing \"a complete evaporation of liquidity\".'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_contexts[train_indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2ddd394",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train_contexts = list(filtered_train_contexts)\n",
    "train_questions_sampled = list(train_questions_sampled)\n",
    "filtered_val_contexts = list(filtered_val_contexts)\n",
    "val_questions_sampled = list(val_questions_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fed5f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(filtered_train_contexts, train_questions_sampled, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(filtered_val_contexts, val_questions_sampled, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e5cdee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForQuestionAnswering, AdamW\n",
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eb3b7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] it threatened the collapse of large financial institutions, which was prevented by the bailout of banks by national governments, but stock markets still dropped worldwide dollars, and a downturn in economic activity leading to the 2008 – 2012 global recession and contributing to the european sovereign - debt crisis the active phase of the crisis, which manifested as a liquidity crisis, can be dated from august 9, 2007, when bnp paribas terminated withdrawals from three hedge funds citing \" a complete evaporation of liquidity \" [SEP] what year did the global recession that followed the financial crisis of 2007 end? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_encodings['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc42f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_pos = encodings.char_to_token(i, answers[i]['answer_start'])\n",
    "        end_pos = encodings.char_to_token(i, answers[i]['answer_end'])\n",
    "\n",
    "        if start_pos is None:\n",
    "            start_pos = tokenizer.model_max_length\n",
    "        if end_pos is None:\n",
    "            shift = 1\n",
    "            while end_pos is None and answers[i]['answer_end'] - shift >= 0:\n",
    "                end_pos = encodings.char_to_token(i, answers[i]['answer_end'] - shift)\n",
    "                shift += 1\n",
    "        if end_pos is None:\n",
    "            end_pos = tokenizer.model_max_length\n",
    "\n",
    "        start_positions.append(start_pos)\n",
    "        end_positions.append(end_pos)\n",
    "        \n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers_sampled)\n",
    "add_token_positions(val_encodings, val_answers_sampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27853df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "630e082b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU: NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# setup GPU/CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Running on GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4750ed6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 313/313 [01:57<00:00,  2.66it/s, loss=1.57]\n",
      "Epoch 1: 100%|██████████| 313/313 [01:59<00:00,  2.62it/s, loss=1.81] \n",
      "Epoch 2: 100%|██████████| 313/313 [01:59<00:00,  2.62it/s, loss=1.36] \n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        start_positions=start_positions,\n",
    "                        end_positions=end_positions)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "221f41b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2744140625"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "acc = []\n",
    "for batch in val_loader:\n",
    "    with torch.no_grad():\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "        acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
    "        acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n",
    "acc = sum(acc)/len(acc)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1146cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def calculate_f1(pred_start, pred_end, true_start, true_end):\n",
    "    pred_tokens = set(range(pred_start, pred_end + 1))\n",
    "    true_tokens = set(range(true_start, true_end + 1))\n",
    "\n",
    "    common_tokens = pred_tokens.intersection(true_tokens)\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0, 0, 0  # precision, recall, f1\n",
    "\n",
    "    precision = len(common_tokens) / len(pred_tokens)\n",
    "    recall = len(common_tokens) / len(true_tokens)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c40b8a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "start_acc = []\n",
    "end_acc = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "for batch in val_loader:\n",
    "    with torch.no_grad():\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "        start_acc.append((start_pred == start_true).sum().item() / len(start_pred))\n",
    "        end_acc.append((end_pred == end_true).sum().item() / len(end_pred))\n",
    "        for sp, ep, st, et in zip(start_pred, end_pred, start_true, end_true):\n",
    "            precision, recall, f1 = calculate_f1(sp.item(), ep.item(), st.item(), et.item())\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1s.append(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5663543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "avg_start_acc = sum(start_acc) / len(start_acc)\n",
    "avg_end_acc = sum(end_acc) / len(end_acc)\n",
    "avg_precision = sum(precisions) / len(precisions)\n",
    "avg_recall = sum(recalls) / len(recalls)\n",
    "avg_f1 = sum(f1s) / len(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "876fa0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Start Position Accuracy: 0.2266\n",
      "Average End Position Accuracy: 0.3223\n",
      "Average Precision: 0.2506\n",
      "Average Recall: 0.3442\n",
      "Average F1 Score: 0.2568\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Average Start Position Accuracy: {avg_start_acc:.4f}\")\n",
    "print(f\"Average End Position Accuracy: {avg_end_acc:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd5fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df221fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8b9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
